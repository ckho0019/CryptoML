{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "train_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "outputs": [],
      "metadata": {
        "id": "OUdrKjcXDDmd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# MODIFY HERE TO YOUR PATH IN GOOGLE DRIVE\r\n",
        "%cd /content/gdrive/MyDrive/CryptoML"
      ],
      "outputs": [],
      "metadata": {
        "id": "9OO2fAyuDFRL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import speck as sp\r\n",
        "import numpy as np\r\n",
        "from pickle import dump\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Input, Reshape, Permute, Add, Flatten, BatchNormalization, Activation\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "from tensorflow.keras.regularizers import l2\r\n",
        "\r\n",
        "bs = 5000;\r\n",
        "dir = './models/'\r\n",
        "\r\n",
        "def cyclic_lr(num_epochs, high_lr, low_lr):\r\n",
        "  res = lambda i: low_lr + ((num_epochs-1) - i % num_epochs)/(num_epochs-1) * (high_lr - low_lr)\r\n",
        "  return res\r\n",
        "\r\n",
        "def make_checkpoint(datei):\r\n",
        "  assert datei[:9] == \"./models/\"\r\n",
        "  assert datei[-3:] == \".h5\"\r\n",
        "    \r\n",
        "  res = ModelCheckpoint(datei, monitor='val_loss', save_best_only = True)\r\n",
        "  return res\r\n",
        "\r\n",
        "# make residual tower of convolutional blocks\r\n",
        "def make_resnet(num_blocks=2, num_filters=32, num_outputs=1, d1=64, d2=64, word_size=16, ks=3, depth=5, reg_param=0.0001, final_activation='sigmoid'):\r\n",
        "  assert num_outputs == 1\r\n",
        "  assert num_blocks in [2,4]\r\n",
        "\r\n",
        "  # Input and preprocessing layers\r\n",
        "  inp = Input(shape=(num_blocks * word_size * 2))\r\n",
        "  rs = Reshape((2 * num_blocks, word_size))(inp)\r\n",
        "  perm = Permute((2,1))(rs)\r\n",
        "  \r\n",
        "  # add a single residual layer that will expand the data to num_filters channels\r\n",
        "  # this is a bit-sliced layer\r\n",
        "  conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm)\r\n",
        "  conv0 = BatchNormalization()(conv0)\r\n",
        "  conv0 = Activation('relu')(conv0)\r\n",
        "\r\n",
        "  # add residual blocks\r\n",
        "  shortcut = conv0\r\n",
        "  for _ in range(depth):\r\n",
        "    conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut)\r\n",
        "    conv1 = BatchNormalization()(conv1)\r\n",
        "    conv1 = Activation('relu')(conv1)\r\n",
        "    conv2 = Conv1D(num_filters, kernel_size=ks, padding='same',kernel_regularizer=l2(reg_param))(conv1)\r\n",
        "    conv2 = BatchNormalization()(conv2)\r\n",
        "    conv2 = Activation('relu')(conv2)\r\n",
        "    shortcut = Add()([shortcut, conv2])\r\n",
        "\r\n",
        "  # add prediction head\r\n",
        "  flat1 = Flatten()(shortcut)\r\n",
        "  dense1 = Dense(d1,kernel_regularizer=l2(reg_param))(flat1)\r\n",
        "  dense1 = BatchNormalization()(dense1)\r\n",
        "  dense1 = Activation('relu')(dense1)\r\n",
        "  dense2 = Dense(d2, kernel_regularizer=l2(reg_param))(dense1)\r\n",
        "  dense2 = BatchNormalization()(dense2)\r\n",
        "  dense2 = Activation('relu')(dense2)\r\n",
        "  out = Dense(num_outputs, activation=final_activation, kernel_regularizer=l2(reg_param))(dense2)\r\n",
        "  model = Model(inputs=inp, outputs=out)\r\n",
        "\r\n",
        "  return model\r\n",
        "\r\n",
        "def train_speck_distinguisher(num_epochs, num_rounds=7, depth=1, num_blocks=2, diffa=(0x0040,0), diffb=(0x0020,0)):\r\n",
        "    assert num_epochs > 0\r\n",
        "    assert num_rounds > 0\r\n",
        "    assert depth > 0\r\n",
        "    assert num_blocks in [2, 4]\r\n",
        "    assert type(diffa) is tuple and type(diffb) is tuple\r\n",
        "\r\n",
        "    # create the network\r\n",
        "    net = make_resnet(num_blocks=num_blocks, depth=depth, reg_param=10**-5)\r\n",
        "    net.compile(optimizer='adam',loss='mse',metrics=['acc'])\r\n",
        "\r\n",
        "    # generate training and validation data\r\n",
        "    if num_blocks == 2:\r\n",
        "      X, Y = sp.make_train_data_2pt(10**7, num_rounds, diff=diffa)\r\n",
        "      X_eval, Y_eval = sp.make_train_data_2pt(10**6, num_rounds, diff=diffa)\r\n",
        "    \r\n",
        "    else:\r\n",
        "      X, Y = sp.make_train_data_4pt(10**7, num_rounds, diffa=diffa, diffb=diffb)\r\n",
        "      X_eval, Y_eval = sp.make_train_data_4pt(10**6, num_rounds, diffa=diffa, diffb=diffb)\r\n",
        "\r\n",
        "    # set up model checkpoint\r\n",
        "    check = make_checkpoint(dir+'best'+str(num_rounds)+'depth'+str(depth)+'.h5')\r\n",
        "\r\n",
        "    # create learnrate schedule\r\n",
        "    lr = LearningRateScheduler(cyclic_lr(10,0.002, 0.0001))\r\n",
        "\r\n",
        "    # train and evaluate\r\n",
        "    h = net.fit(X,Y,epochs=num_epochs,batch_size=bs,validation_data=(X_eval, Y_eval), callbacks=[lr,check])\r\n",
        "    np.save(dir+'h'+str(num_rounds)+'r_depth'+str(depth)+'.npy', h.history['val_acc'])\r\n",
        "    np.save(dir+'h'+str(num_rounds)+'r_depth'+str(depth)+'.npy', h.history['val_loss'])\r\n",
        "    dump(h.history,open(dir+'hist'+str(num_rounds)+'r_depth'+str(depth)+'.p','wb'))\r\n",
        "    print(\"Best validation accuracy: \", np.max(h.history['val_acc']))\r\n",
        "\r\n",
        "    return net, h"
      ],
      "outputs": [],
      "metadata": {
        "id": "fhpoMEFWXywa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# MODIFY HERE: \r\n",
        "\r\n",
        "# number of epochs, number of Speck rounds, depth, number of plaintexts (2 or 4), bit difference #1, bit difference #2 (if 4 plaintexts)\r\n",
        "train_speck_distinguisher(1, num_rounds=5, depth=10, num_blocks=4, diffa=(0x0040,0), diffb=(0x0020,0))"
      ],
      "outputs": [],
      "metadata": {
        "id": "8kX49J7fDx4-"
      }
    }
  ]
}